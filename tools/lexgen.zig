const std = @import("std");

const TokenDef = struct {
    name: []const u8,
    regex: []const u8,
    scope: ?[]const u8 = null,
};

pub fn main() !void {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer gpa.deinit();
    const allocator = gpa.allocator();

    const token_data = try std.fs.cwd().readFileAlloc(allocator, "tokens.json", 1024 * 1024);
    defer allocator.free(token_data);

    var parsed = try std.json.parseFromSlice([]TokenDef, allocator, token_data, .{});
    defer parsed.deinit();

    const out_dir = try std.fs.cwd().makePath("src/lexer");
    const out_file = try std.fs.cwd().createFile("src/lexer/dfa_table.h", .{ .truncate = true });
    defer out_file.close();
    var w = out_file.writer();

    try w.writeAll("// Auto-generated by lexgen.zig\n");
    try w.writeAll("#ifndef DFA_TABLE_H\n#define DFA_TABLE_H\n\n");
    try w.writeAll("typedef enum {\n");
    for (parsed.value, 0..) |tok, idx| {
        const upper = std.ascii.upperString(allocator, tok.name) catch tok.name;
        defer if (upper.ptr != tok.name.ptr) allocator.free(upper);
        try w.print("    TOKEN_{s} = {d},\n", .{ upper, idx });
    }
    try w.writeAll("    TOKEN_EOF,\n    TOKEN_UNKNOWN\n} TokenType;\n\n");
    // Placeholder DFA table
    try w.writeAll("static const uint16_t DFA[][256] = {\n    {0}\n};\n\n");
    try w.writeAll("#endif // DFA_TABLE_H\n");
}
